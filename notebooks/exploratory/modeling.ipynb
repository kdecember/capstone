{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/kyledecember1/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kyledecember1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign paths for csv data\n",
    "\n",
    "reviews_path = os.path.join(os.pardir, os.pardir, 'data/reviews.csv')\n",
    "games_path = os.path.join(os.pardir, os.pardir, 'data/games.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes from csv files\n",
    "\n",
    "df_reviews = pd.read_csv(reviews_path)\n",
    "df_games = pd.read_csv(games_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "\n",
    "df_reviews.drop(['found_funny', 'compensation', 'user_id', 'Unnamed: 0', 'products', 'page_order',\\\n",
    "                'date', 'early_access', 'page'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a frequency column based on product_id, sort by said column\n",
    "\n",
    "df_reviews['freq'] = df_reviews.groupby('product_id')['product_id'].transform('count')\n",
    "df_reviews.sort_values(by=['freq', 'product_id'], ascending=[False, True], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove null values\n",
    "\n",
    "df_reviews.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove reviews by users that had under 1 hour played for the game\n",
    "# remove games that have less than 500 total reviews\n",
    "\n",
    "df_reviews = df_reviews[df_reviews['hours'] >= 1]\n",
    "df_reviews = df_reviews[df_reviews['freq'] >= 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take subsample of data for text manipulation/modeling purposes\n",
    "\n",
    "df_sample = df_reviews.sample(axis=0, n=250000)\n",
    "df_sample.sort_values(by=['freq', 'product_id'], ascending=[False, True], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make lowercase\n",
    "\n",
    "df_sample['text'] = df_sample['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove new line indicators\n",
    "\n",
    "df_sample['text'] = df_sample['text'].str.replace('\\n', ' ')\n",
    "df_sample['text'] = df_sample['text'].str.replace('.\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3965943    [i, am, stunned, by, what, valve, has, pulled,...\n",
       "3831628    [people, can, be, rude, and, it, may, seem, a,...\n",
       "3899176    [pros, :, -, class, variety, -, many, differen...\n",
       "3915021                                 [its, a, good, game]\n",
       "3600448    [i, liked, this, game, since, i, first, played...\n",
       "                                 ...                        \n",
       "6627576                                         [good, game]\n",
       "6626028    [why, do, u, guys, have, to, hate, every, thin...\n",
       "6630862                                               [good]\n",
       "6628347    [the, game, does, have, some, bugs, inb, it, b...\n",
       "6629891                                                [rip]\n",
       "Name: tokens, Length: 250000, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize text\n",
    "\n",
    "df_sample['tokens'] = df_sample['text'].apply(nltk.word_tokenize)\n",
    "df_sample['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join tokens into single string\n",
    "\n",
    "df_sample['clean_text'] = df_sample['tokens'].apply(', '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3965943    i, am, stunned, by, what, valve, has, pulled, ...\n",
       "3831628    people, can, be, rude, and, it, may, seem, a, ...\n",
       "3899176    pros, :, -, class, variety, -, many, different...\n",
       "3915021                                   its, a, good, game\n",
       "3600448    i, liked, this, game, since, i, first, played, it\n",
       "                                 ...                        \n",
       "6627576                                           good, game\n",
       "6626028    why, do, u, guys, have, to, hate, every, thing...\n",
       "6630862                                                 good\n",
       "6628347    the, game, does, have, some, bugs, inb, it, bu...\n",
       "6629891                                                  rip\n",
       "Name: clean_text, Length: 250000, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['clean_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling\n",
    "\n",
    "** attempts to improve from FSM that simply recommended the most reviewed games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_sample, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged = train.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['clean_text']), tags=[r['product_id']]), axis=1)\n",
    "test_tagged = test.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['clean_text']), tags=[r['product_id']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['if', 'you', 'are', 'even', 'remotely', 'fan', 'of', 'star', 'trek', 'heck', 'if', 'you', 'have', 'even', 'ever', 'see', 'an', 'episode', 'you', 'need', 'to', 'try', 'this', 'game', 'did', \"n't\", 'try', 'it', 'for', 'the', 'longest', 'time', 'because', 'assumed', 'the', 'producation', 'quality', 'was', 'poor', 'and', 'would', \"n't\", 'be', 'able', 'to', 'get', 'into', 'it', 'was', 'so', 'wrong', 'can', 'not', 'stop', 'playing', 'this', 'game', 'do', 'yourself', 'favor', 'and', 'give', 'it', 'try', 'it', 'is', 'amazing', 'the', 'customization', 'is', 'incredible', 'and', 'the', 'gameplay', 'is', 'very', 'well', 'done', 'want', 'to', 'be', 'ferengi', 'captain', 'and', 'have', 'crew', 'of', 'nothing', 'but', 'ferengis', 'you', 'can', 'do', 'that', 'want', 'to', 'captain', 'the', 'original', 'enterprise', 'and', 'make', 'character', 'that', 'looks', 'like', 'captain', 'kirk', 'you', 'can', 'do', 'that', 'too', 'long', 'story', 'short', 'try', 'this', 'game', 'and', 'put', 'some', 'time', 'into', 'the', 'creation', 'of', 'your', 'character', 'and', 'crew', 'the', 'number', 'of', 'possibilities', 'are', 'amazing'], tags=[9900])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged.values[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175000/175000 [00:00<00:00, 1700400.08it/s]\n"
     ]
    }
   ],
   "source": [
    "model1 = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model1.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.train(train_tagged, total_examples=len(train_tagged), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model1.save('model1.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_for_learning(model, input_docs):\n",
    "    sents = input_docs\n",
    "    targets, feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
